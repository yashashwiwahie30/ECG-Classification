{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "036613ba",
   "metadata": {},
   "source": [
    "Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b54a93e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e4608e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.10.0'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5857cce9",
   "metadata": {},
   "source": [
    "# Part 1 - Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979115de",
   "metadata": {},
   "source": [
    "Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b289d421",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('ECG_NAGPUR-2.csv')\n",
    "X = dataset.iloc[:, 0:-1].values\n",
    "y = dataset.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5c3fdea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.40677966 0.4039548  0.40112994 ... 0.71186441 0.70338983 0.69774011]\n",
      " [0.71125265 0.70912951 0.71125265 ... 0.91932059 0.92569002 0.92569002]\n",
      " [0.43808256 0.43808256 0.43408788 ... 0.39147803 0.38881491 0.38881491]\n",
      " ...\n",
      " [0.32939189 0.32922297 0.33783784 ... 0.34695946 0.34851351 0.34364865]\n",
      " [0.88132417 0.88194878 0.88257339 ... 0.63678951 0.63678326 0.61905059]\n",
      " [0.64961569 0.64988073 0.65014577 ... 0.44772595 0.42646965 0.426467  ]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bc9480b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 1 0 0 1 1 1 1 1 1 1 0 1 0 1 0 0 1 1 1 1 0 1 0 0 1 0 1 0 0 1 0 1 1 0\n",
      " 1 0 1 0 1 1 0 0 1 0 0 0 1 1 1 1 1 1 0 0 1 0 0 0 0 0 0 1 1 1 0 1 0 1 1 1 1\n",
      " 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 0 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1\n",
      " 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1\n",
      " 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 1 0 1 1 1 0 0 0\n",
      " 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc40a94",
   "metadata": {},
   "source": [
    "Splitting the dataset into the Training set and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b99cec5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12c42eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\shash\\.conda\\envs\\tf\\lib\\site-packages (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\shash\\.conda\\envs\\tf\\lib\\site-packages (from scikit-learn) (1.23.5)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\shash\\.conda\\envs\\tf\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\shash\\.conda\\envs\\tf\\lib\\site-packages (from scikit-learn) (1.9.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\shash\\.conda\\envs\\tf\\lib\\site-packages (from scikit-learn) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f972937",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2094b0c8",
   "metadata": {},
   "source": [
    "Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bcc50d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22a13b0",
   "metadata": {},
   "source": [
    "# Part 2 - Building the ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9395e7",
   "metadata": {},
   "source": [
    "Initializing the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3231a9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ann = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4e3d32",
   "metadata": {},
   "source": [
    "Adding the input layer and the first hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54da703c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.add(tf.keras.layers.Dense(units=6, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8637ad6",
   "metadata": {},
   "source": [
    "Adding the second hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff24077a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.add(tf.keras.layers.Dense(units=6, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf8d729",
   "metadata": {},
   "source": [
    "Adding the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11360ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9229665",
   "metadata": {},
   "source": [
    "# Part 3 - Training the ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456c8c06",
   "metadata": {},
   "source": [
    "Compiling the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cdf3a765",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e3b8e5",
   "metadata": {},
   "source": [
    "Training the ANN on the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aba42f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "7/7 [==============================] - 1s 3ms/step - loss: 0.7514 - accuracy: 0.5952\n",
      "Epoch 2/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6096 - accuracy: 0.7000\n",
      "Epoch 3/100\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.5663 - accuracy: 0.7524\n",
      "Epoch 4/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5442 - accuracy: 0.7619\n",
      "Epoch 5/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5344 - accuracy: 0.7714\n",
      "Epoch 6/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.5241 - accuracy: 0.7810\n",
      "Epoch 7/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5163 - accuracy: 0.7667\n",
      "Epoch 8/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.5081 - accuracy: 0.7714\n",
      "Epoch 9/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.5024 - accuracy: 0.7762\n",
      "Epoch 10/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4926 - accuracy: 0.7857\n",
      "Epoch 11/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.4846 - accuracy: 0.7667\n",
      "Epoch 12/100\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.4750 - accuracy: 0.7857\n",
      "Epoch 13/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4622 - accuracy: 0.7857\n",
      "Epoch 14/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4486 - accuracy: 0.7952\n",
      "Epoch 15/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.4456 - accuracy: 0.7762\n",
      "Epoch 16/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4293 - accuracy: 0.7905\n",
      "Epoch 17/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4202 - accuracy: 0.7857\n",
      "Epoch 18/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4082 - accuracy: 0.8000\n",
      "Epoch 19/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.4014 - accuracy: 0.7810\n",
      "Epoch 20/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.3985 - accuracy: 0.7905\n",
      "Epoch 21/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.3915 - accuracy: 0.7905\n",
      "Epoch 22/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3888 - accuracy: 0.7905\n",
      "Epoch 23/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.3767 - accuracy: 0.8000\n",
      "Epoch 24/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3585 - accuracy: 0.8095\n",
      "Epoch 25/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.3769 - accuracy: 0.7810\n",
      "Epoch 26/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3723 - accuracy: 0.7952\n",
      "Epoch 27/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3672 - accuracy: 0.7952\n",
      "Epoch 28/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.3584 - accuracy: 0.7952\n",
      "Epoch 29/100\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.3510 - accuracy: 0.8000\n",
      "Epoch 30/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.3463 - accuracy: 0.8095\n",
      "Epoch 31/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3435 - accuracy: 0.8143\n",
      "Epoch 32/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.3500 - accuracy: 0.8000\n",
      "Epoch 33/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3495 - accuracy: 0.8000\n",
      "Epoch 34/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.3480 - accuracy: 0.8048\n",
      "Epoch 35/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.3408 - accuracy: 0.8048\n",
      "Epoch 36/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3404 - accuracy: 0.8000\n",
      "Epoch 37/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.3366 - accuracy: 0.8000\n",
      "Epoch 38/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3396 - accuracy: 0.8048\n",
      "Epoch 39/100\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.3450 - accuracy: 0.7952\n",
      "Epoch 40/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3429 - accuracy: 0.8048\n",
      "Epoch 41/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.3390 - accuracy: 0.8095\n",
      "Epoch 42/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3349 - accuracy: 0.8143\n",
      "Epoch 43/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.3309 - accuracy: 0.8095\n",
      "Epoch 44/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3558 - accuracy: 0.7429\n",
      "Epoch 45/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3308 - accuracy: 0.7857\n",
      "Epoch 46/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3349 - accuracy: 0.7857\n",
      "Epoch 47/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3309 - accuracy: 0.8095\n",
      "Epoch 48/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.3294 - accuracy: 0.8048\n",
      "Epoch 49/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3283 - accuracy: 0.8048\n",
      "Epoch 50/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.3262 - accuracy: 0.8095\n",
      "Epoch 51/100\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.3243 - accuracy: 0.8143\n",
      "Epoch 52/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3233 - accuracy: 0.8190\n",
      "Epoch 53/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.3228 - accuracy: 0.8238\n",
      "Epoch 54/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.3215 - accuracy: 0.8238\n",
      "Epoch 55/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.3197 - accuracy: 0.8238\n",
      "Epoch 56/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3176 - accuracy: 0.8238\n",
      "Epoch 57/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3160 - accuracy: 0.8333\n",
      "Epoch 58/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.3145 - accuracy: 0.8286\n",
      "Epoch 59/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.3120 - accuracy: 0.8381\n",
      "Epoch 60/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3092 - accuracy: 0.8381\n",
      "Epoch 61/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3058 - accuracy: 0.8238\n",
      "Epoch 62/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.3069 - accuracy: 0.8429\n",
      "Epoch 63/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.2998 - accuracy: 0.8381\n",
      "Epoch 64/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.3110 - accuracy: 0.8190\n",
      "Epoch 65/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3006 - accuracy: 0.8095\n",
      "Epoch 66/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3098 - accuracy: 0.8095\n",
      "Epoch 67/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3382 - accuracy: 0.7952\n",
      "Epoch 68/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.3933 - accuracy: 0.8000\n",
      "Epoch 69/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3209 - accuracy: 0.8000\n",
      "Epoch 70/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3141 - accuracy: 0.8238\n",
      "Epoch 71/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.3185 - accuracy: 0.8190\n",
      "Epoch 72/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3159 - accuracy: 0.8095\n",
      "Epoch 73/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.3152 - accuracy: 0.8095\n",
      "Epoch 74/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3115 - accuracy: 0.8190\n",
      "Epoch 75/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.3050 - accuracy: 0.8190\n",
      "Epoch 76/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3007 - accuracy: 0.8238\n",
      "Epoch 77/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.2999 - accuracy: 0.8286\n",
      "Epoch 78/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.2966 - accuracy: 0.8333\n",
      "Epoch 79/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.2969 - accuracy: 0.8190\n",
      "Epoch 80/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3091 - accuracy: 0.8143\n",
      "Epoch 81/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3095 - accuracy: 0.8143\n",
      "Epoch 82/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.3070 - accuracy: 0.8238\n",
      "Epoch 83/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.3009 - accuracy: 0.8381\n",
      "Epoch 84/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.2941 - accuracy: 0.8429\n",
      "Epoch 85/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.2833 - accuracy: 0.8619\n",
      "Epoch 86/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.2908 - accuracy: 0.8429\n",
      "Epoch 87/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.2920 - accuracy: 0.8476\n",
      "Epoch 88/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.2911 - accuracy: 0.8429\n",
      "Epoch 89/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.2829 - accuracy: 0.8667\n",
      "Epoch 90/100\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.2972 - accuracy: 0.8190\n",
      "Epoch 91/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.2896 - accuracy: 0.8476\n",
      "Epoch 92/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.2950 - accuracy: 0.8381\n",
      "Epoch 93/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.2925 - accuracy: 0.8381\n",
      "Epoch 94/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.2846 - accuracy: 0.8524\n",
      "Epoch 95/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.2752 - accuracy: 0.8667\n",
      "Epoch 96/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.2664 - accuracy: 0.8762\n",
      "Epoch 97/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.2637 - accuracy: 0.8667\n",
      "Epoch 98/100\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.2703 - accuracy: 0.8667\n",
      "Epoch 99/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.2720 - accuracy: 0.8571\n",
      "Epoch 100/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.2579 - accuracy: 0.8810\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b1d2a8bb50>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann.fit(X_train, y_train, batch_size = 32, epochs = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6f92fd",
   "metadata": {},
   "source": [
    "# Part 4 - Making the predictions and evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a51ef1",
   "metadata": {},
   "source": [
    "Predicting the Test set results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "afd79c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 0s/step\n",
      "[[1 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 1]\n",
      " [0 1]\n",
      " [1 1]\n",
      " [0 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = ann.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)\n",
    "print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f5f70d",
   "metadata": {},
   "source": [
    "Making the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "450669e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4  4]\n",
      " [10 35]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7358490566037735"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ee3db8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://b7efc228-36c8-4f51-b168-58fd1d782c57/assets\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "pickle.dump(ann,open('ECG_Phase2_model_KNN.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a276c3d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'flask'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mflask\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Flask, render_template, request\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mprediction_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'flask'"
     ]
    }
   ],
   "source": [
    "from flask import Flask, render_template, request\n",
    "from prediction_utils import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bdff4d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "36aae226",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Unsuccessful TensorSliceReader constructor: Failed to find any matching files for ram://00d0592e-e9ed-4f1d-a9e7-5fa3c3428845/variables/variables\n You may be trying to load on a different device from the computational device. Consider setting the `experimental_io_device` option in `tf.saved_model.LoadOptions` to the io_device such as '/job:localhost'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m d\u001b[38;5;241m=\u001b[39m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_pickle\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mECG_Phase2_model_KNN.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(d)\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\pandas\\io\\pickle.py:208\u001b[0m, in \u001b[0;36mread_pickle\u001b[1;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m warnings\u001b[38;5;241m.\u001b[39mcatch_warnings(record\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    206\u001b[0m         \u001b[38;5;66;03m# We want to silence any warnings about, e.g. moved modules.\u001b[39;00m\n\u001b[0;32m    207\u001b[0m         warnings\u001b[38;5;241m.\u001b[39msimplefilter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mWarning\u001b[39;00m)\n\u001b[1;32m--> 208\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandles\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m excs_to_catch:\n\u001b[0;32m    210\u001b[0m     \u001b[38;5;66;03m# e.g.\u001b[39;00m\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;66;03m#  \"No module named 'pandas.core.sparse.series'\"\u001b[39;00m\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;66;03m#  \"Can't get attribute '__nat_unpickle' on <module 'pandas._libs.tslib\"\u001b[39;00m\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pc\u001b[38;5;241m.\u001b[39mload(handles\u001b[38;5;241m.\u001b[39mhandle, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\keras\\saving\\pickle_utils.py:47\u001b[0m, in \u001b[0;36mdeserialize_model_from_bytecode\u001b[1;34m(serialized_model)\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mGFile(dest_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     46\u001b[0m                 f\u001b[38;5;241m.\u001b[39mwrite(archive\u001b[38;5;241m.\u001b[39mextractfile(name)\u001b[38;5;241m.\u001b[39mread())\n\u001b[1;32m---> 47\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43msave_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemp_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mrmtree(temp_dir)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py:933\u001b[0m, in \u001b[0;36mload_partial\u001b[1;34m(export_dir, filters, tags, options)\u001b[0m\n\u001b[0;32m    930\u001b[0m   loader \u001b[38;5;241m=\u001b[39m Loader(object_graph_proto, saved_model_proto, export_dir,\n\u001b[0;32m    931\u001b[0m                   ckpt_options, options, filters)\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mNotFoundError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 933\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[0;32m    934\u001b[0m       \u001b[38;5;28mstr\u001b[39m(err) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m You may be trying to load on a different device \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    935\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom the computational device. Consider setting the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    936\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`experimental_io_device` option in `tf.saved_model.LoadOptions` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    937\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto the io_device such as \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/job:localhost\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    938\u001b[0m root \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    939\u001b[0m root\u001b[38;5;241m.\u001b[39mgraph_debug_info \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39madjust_debug_info_func_names(debug_info)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for ram://00d0592e-e9ed-4f1d-a9e7-5fa3c3428845/variables/variables\n You may be trying to load on a different device from the computational device. Consider setting the `experimental_io_device` option in `tf.saved_model.LoadOptions` to the io_device such as '/job:localhost'."
     ]
    }
   ],
   "source": [
    "d=pd.read_pickle(\"ECG_Phase2_model_KNN.pkl\")\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5923a6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           C1        C2        C3        C4        C5        C6        C7  \\\n",
      "0    0.406780  0.403955  0.401130  0.401130  0.401130  0.395480  0.395480   \n",
      "1    0.711253  0.709130  0.711253  0.711253  0.707006  0.707006  0.709130   \n",
      "2    0.438083  0.438083  0.434088  0.431425  0.434088  0.435419  0.435419   \n",
      "3    0.916667  0.913462  0.923077  0.929487  0.939103  0.929487  0.919872   \n",
      "4    0.748691  0.746073  0.738220  0.740838  0.748691  0.746073  0.746073   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "258  0.412527  0.410367  0.410151  0.408207  0.403888  0.406048  0.403672   \n",
      "259  0.260456  0.257605  0.258555  0.258460  0.259506  0.258365  0.253802   \n",
      "260  0.329392  0.329223  0.337838  0.344595  0.353041  0.363176  0.380068   \n",
      "261  0.881324  0.881949  0.882573  0.887570  0.888195  0.888819  0.883198   \n",
      "262  0.649616  0.649881  0.650146  0.639014  0.636364  0.636629  0.623112   \n",
      "\n",
      "           C8        C9       C10  ...     C4992     C4993     C4994  \\\n",
      "0    0.392655  0.395480  0.395480  ...  0.737288  0.742938  0.737288   \n",
      "1    0.707006  0.707006  0.709130  ...  0.900212  0.904459  0.908705   \n",
      "2    0.436751  0.439414  0.436751  ...  0.384820  0.384820  0.386152   \n",
      "3    0.916667  0.916667  0.919872  ...  0.846154  0.833333  0.833333   \n",
      "4    0.738220  0.740838  0.751309  ...  0.769634  0.767016  0.764398   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "258  0.405832  0.407991  0.405616  ...  0.226566  0.263499  0.298056   \n",
      "259  0.251901  0.255703  0.255608  ...  0.323479  0.327376  0.331559   \n",
      "260  0.405405  0.422297  0.454392  ...  0.335236  0.342128  0.343666   \n",
      "261  0.889444  0.893816  0.890069  ...  0.630625  0.630618  0.636802   \n",
      "262  0.609860  0.596607  0.593957  ...  0.447729  0.431633  0.436920   \n",
      "\n",
      "        C4995     C4996     C4997     C4998     C4999     C5000  Class  \n",
      "0    0.734463  0.728814  0.723164  0.711864  0.703390  0.697740      1  \n",
      "1    0.912951  0.915074  0.915074  0.919321  0.925690  0.925690      1  \n",
      "2    0.388815  0.392810  0.395473  0.391478  0.388815  0.388815      0  \n",
      "3    0.839744  0.836538  0.833333  0.833333  0.830128  0.836538      1  \n",
      "4    0.772251  0.774869  0.777487  0.782723  0.787958  0.787958      0  \n",
      "..        ...       ...       ...       ...       ...       ...    ...  \n",
      "258  0.323758  0.342981  0.360324  0.378661  0.390700  0.399171      0  \n",
      "259  0.331654  0.331882  0.338308  0.339496  0.342966  0.346103      1  \n",
      "260  0.342111  0.342095  0.345186  0.346959  0.348514  0.343649      1  \n",
      "261  0.630612  0.636796  0.630606  0.636790  0.636783  0.619051      1  \n",
      "262  0.444951  0.453051  0.463265  0.447726  0.426470  0.426467      0  \n",
      "\n",
      "[263 rows x 5001 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('ECG_NAGPUR-2.csv')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c9c4af9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Numpy Array\n",
      "----------\n",
      " [[0.40677966 0.4039548  0.40112994 ... 0.70338983 0.69774011 1.        ]\n",
      " [0.71125265 0.70912951 0.71125265 ... 0.92569002 0.92569002 1.        ]\n",
      " [0.43808256 0.43808256 0.43408788 ... 0.38881491 0.38881491 0.        ]\n",
      " ...\n",
      " [0.32939189 0.32922297 0.33783784 ... 0.34851351 0.34364865 1.        ]\n",
      " [0.88132417 0.88194878 0.88257339 ... 0.63678326 0.61905059 1.        ]\n",
      " [0.64961569 0.64988073 0.65014577 ... 0.42646965 0.426467   0.        ]]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "arr = df.to_numpy()\n",
    "print('\\nNumpy Array\\n----------\\n', arr)\n",
    "print(type(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "56b2f4c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Omkar => {'key': 'Omkar', 'name': 'Omkar Pathak', 'age': 21, 'pay': 40000}\n",
      "Jagdish => {'key': 'Jagdish', 'name': 'Jagdish Pathak', 'age': 50, 'pay': 50000}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "def storeData():\n",
    "    # initializing data to be stored in db\n",
    "    Omkar = {'key' : 'Omkar', 'name' : 'Omkar Pathak',\n",
    "    'age' : 21, 'pay' : 40000}\n",
    "    Jagdish = {'key' : 'Jagdish', 'name' : 'Jagdish Pathak',\n",
    "    'age' : 50, 'pay' : 50000}\n",
    "  \n",
    "    # database\n",
    "    db = {}\n",
    "    db['Omkar'] = Omkar\n",
    "    db['Jagdish'] = Jagdish\n",
    "      \n",
    "    # Its important to use binary mode\n",
    "    dbfile = open('examplePickle', 'ab')\n",
    "      \n",
    "    # source, destination\n",
    "    pickle.dump(db, dbfile)                     \n",
    "    dbfile.close()\n",
    "  \n",
    "def loadData():\n",
    "    # for reading also binary mode is important\n",
    "    dbfile = open('examplePickle', 'rb')     \n",
    "    db = pickle.load(dbfile)\n",
    "    for keys in db:\n",
    "        print(keys, '=>', db[keys])\n",
    "    dbfile.close()\n",
    "  \n",
    "if __name__ == '__main__':\n",
    "    storeData()\n",
    "    loadData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "75beebd1",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ECG_Phase2_model_KNN (1).pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m obj \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mECG_Phase2_model_KNN (1).pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m     )\n\u001b[1;32m--> 282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ECG_Phase2_model_KNN (1).pickle'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "obj = pickle.load(open(\"ECG_Phase2_model_KNN (1).pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ad4d7a5c",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ECG_Phase2_model_KNN (1).pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpprint\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m obj \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mECG_Phase2_model_KNN (1).pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      5\u001b[0m      pprint\u001b[38;5;241m.\u001b[39mpprint(obj, stream\u001b[38;5;241m=\u001b[39mf)\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m     )\n\u001b[1;32m--> 282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ECG_Phase2_model_KNN (1).pickle'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pprint\n",
    "obj = pickle.load(open(\"ECG_Phase2_model_KNN (1).pickle\", \"rb\"))\n",
    "with open(\"out.txt\", \"a\") as f:\n",
    "     pprint.pprint(obj, stream=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b0f01151",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (3393555924.py, line 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[52], line 15\u001b[1;36m\u001b[0m\n\u001b[1;33m    return render_template('prediction.html', name=name, -----=f\"{model_prediction}%\")\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, render_template, request\n",
    "from prediction_utils import *\n",
    "\n",
    "model = pickle.load(open('ECG_Phase2_model_KNN.pkl','rb'))\n",
    "\n",
    "app = Flask(__name__)\n",
    " \n",
    "    for(0<i<4999)\n",
    "    v1 = \n",
    "    v2 = str(i+1)\n",
    "    v1 + v2 =key\n",
    "    \n",
    "    arr = df.to_numpy()\n",
    "print('\\nNumpy Array\\n----------\\n', arr)\n",
    "print(type(arr))\n",
    "\n",
    "    arr = np.loadtxt(\"ECG_NAGPUR-2.csv\", delimiter=\",\")\n",
    "minimum=arr.min()\n",
    "arr1=arr-minimum\n",
    "maximum=arr1.max()\n",
    "arr1=arr1/maximum\n",
    "arr1.tofile('ECG_NAGPUR-2_normalised.csv', sep = ',')\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model_prediction = predict()\n",
    "  return render_template('prediction.html', name=name, -----=f\"{model_prediction}%\")\n",
    "\n",
    "\n",
    "app.run(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "980cdc18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flask\n",
      "  Downloading Flask-2.2.2-py3-none-any.whl (101 kB)\n",
      "     -------------------------------------- 101.5/101.5 kB 2.9 MB/s eta 0:00:00\n",
      "Collecting itsdangerous>=2.0\n",
      "  Downloading itsdangerous-2.1.2-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: Werkzeug>=2.2.2 in c:\\users\\shash\\.conda\\envs\\tf\\lib\\site-packages (from flask) (2.2.2)\n",
      "Requirement already satisfied: Jinja2>=3.0 in c:\\users\\shash\\.conda\\envs\\tf\\lib\\site-packages (from flask) (3.1.2)\n",
      "Requirement already satisfied: importlib-metadata>=3.6.0 in c:\\users\\shash\\.conda\\envs\\tf\\lib\\site-packages (from flask) (4.11.3)\n",
      "Requirement already satisfied: click>=8.0 in c:\\users\\shash\\.conda\\envs\\tf\\lib\\site-packages (from flask) (8.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\shash\\.conda\\envs\\tf\\lib\\site-packages (from click>=8.0->flask) (0.4.6)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\shash\\.conda\\envs\\tf\\lib\\site-packages (from importlib-metadata>=3.6.0->flask) (3.11.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\shash\\.conda\\envs\\tf\\lib\\site-packages (from Jinja2>=3.0->flask) (2.1.1)\n",
      "Installing collected packages: itsdangerous, flask\n",
      "Successfully installed flask-2.2.2 itsdangerous-2.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install flask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0c46b4af",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Unsuccessful TensorSliceReader constructor: Failed to find any matching files for ram://41631fdc-0688-4766-8922-b6c65498518c/variables/variables\n You may be trying to load on a different device from the computational device. Consider setting the `experimental_io_device` option in `tf.saved_model.LoadOptions` to the io_device such as '/job:localhost'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[63], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mECG_Phase2_model_KNN.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m app \u001b[38;5;241m=\u001b[39m Flask(_name_)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;129m@app\u001b[39m\u001b[38;5;241m.\u001b[39mroute(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mindex\u001b[39m():\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\keras\\saving\\pickle_utils.py:47\u001b[0m, in \u001b[0;36mdeserialize_model_from_bytecode\u001b[1;34m(serialized_model)\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mGFile(dest_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     46\u001b[0m                 f\u001b[38;5;241m.\u001b[39mwrite(archive\u001b[38;5;241m.\u001b[39mextractfile(name)\u001b[38;5;241m.\u001b[39mread())\n\u001b[1;32m---> 47\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43msave_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemp_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mrmtree(temp_dir)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py:933\u001b[0m, in \u001b[0;36mload_partial\u001b[1;34m(export_dir, filters, tags, options)\u001b[0m\n\u001b[0;32m    930\u001b[0m   loader \u001b[38;5;241m=\u001b[39m Loader(object_graph_proto, saved_model_proto, export_dir,\n\u001b[0;32m    931\u001b[0m                   ckpt_options, options, filters)\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mNotFoundError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 933\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[0;32m    934\u001b[0m       \u001b[38;5;28mstr\u001b[39m(err) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m You may be trying to load on a different device \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    935\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom the computational device. Consider setting the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    936\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`experimental_io_device` option in `tf.saved_model.LoadOptions` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    937\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto the io_device such as \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/job:localhost\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    938\u001b[0m root \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    939\u001b[0m root\u001b[38;5;241m.\u001b[39mgraph_debug_info \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39madjust_debug_info_func_names(debug_info)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for ram://41631fdc-0688-4766-8922-b6c65498518c/variables/variables\n You may be trying to load on a different device from the computational device. Consider setting the `experimental_io_device` option in `tf.saved_model.LoadOptions` to the io_device such as '/job:localhost'."
     ]
    }
   ],
   "source": [
    "from flask import Flask,request,jsonify\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "model = pickle.load(open('ECG_Phase2_model_KNN.pkl','rb'))\n",
    "app = Flask(_name_)\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return \"Hello world\"\n",
    "@app.route('/predict',methods=['GET','POST'])\n",
    "def predict():\n",
    "    coordinate = request.form.get('coordinate')\n",
    "    coordinates=[]\n",
    "    for i in coordinate.split():\n",
    "        i=int(i)\n",
    "        coordinates.append(i)\n",
    "    arr1 = np.array(coordinates)\n",
    "    print(arr1)\n",
    "    minimum = arr1.min()\n",
    "    prr1 = arr1 - minimum\n",
    "    maximum = prr1.max()\n",
    "    prr1 = prr1 / maximum\n",
    "    result = model.predict([prr1])[0]\n",
    "    return jsonify({'checking':str(result)})\n",
    "if _name_ == '_main_':\n",
    "    app.run(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfce0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask,re"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
